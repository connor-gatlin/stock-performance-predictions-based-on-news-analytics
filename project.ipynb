{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from itertools import chain\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import datetime\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(2018)\n",
    "stop = set(stopwords.words('english'))\n",
    "py.init_notebook_mode(connected=True)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "99126ff18d939122d6f768b5d17006fdbdf429a1"
   },
   "outputs": [],
   "source": [
    "# turn off analytics for training, or running out of memory.\n",
    "ANALYTICS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "70b687da901d120c4244a15f13775ee59fe54d79"
   },
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "1ee6824cf41c4fd03be113d54e6975cf3574c06f"
   },
   "outputs": [],
   "source": [
    "market_train_df = pd.read_csv('marketdata.csv')\n",
    "news_train_df = pd.read_csv('newsdata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a21a4bd4a0be55614fcdcbc8f1812b5994dabb19"
   },
   "source": [
    "### Market Data\n",
    "\n",
    "The data includes a subset of US-listed instruments. The set of included instruments changes daily and is determined based on the amount traded and the availability of information. This means that there may be instruments that enter and leave this subset of data. There may therefore be gaps in the data provided, and this does not necessarily imply that that data does not exist (those rows are likely not included due to the selection criteria).\n",
    "\n",
    "The marketdata contains a variety of returns calculated over different timespans. All of the returns in this set of marketdata have these properties:\n",
    "\n",
    "Returns are always calculated either open-to-open (from the opening time of one trading day to the open of another) or close-to-close (from the closing time of one trading day to the open of another).\n",
    "Returns are either raw, meaning that the data is not adjusted against any benchmark, or market-residualized (Mktres), meaning that the movement of the market as a whole has been accounted for, leaving only movements inherent to the instrument.\n",
    "Returns can be calculated over any arbitrary interval. Provided here are 1 day and 10 day horizons.\n",
    "Returns are tagged with 'Prev' if they are backwards looking in time, or 'Next' if forwards looking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "cca04f17e12924251a4ea85f1ea63a81e4c1c4eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4072956, 17)\n"
     ]
    }
   ],
   "source": [
    "print(market_train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "a10641e3f980559a25ebc3ab83082443ce344de8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    market_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "b16ba72f2a8bd1b40fbc7ed39b3863933891358f"
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    market_train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "68399eec91a6c1e773c9534969949ca40f04f290"
   },
   "source": [
    "From the price history of 5 randomly picked assets below, we learn that securities could have dramatically different lengths of history in this data set. Learning from securities with longer history is probably more meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "e49f9296ce4c92adc8b82bc4e96fe05d1a1845d7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    data = []\n",
    "    for asset in np.random.choice(market_train_df['assetName'].unique(), 5):\n",
    "        df = market_train_df[(market_train_df['assetName'] == asset)]\n",
    "\n",
    "        data.append(go.Scatter(\n",
    "            x = df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n",
    "            y = df['close'].values,\n",
    "            name = asset\n",
    "        ))\n",
    "    layout = go.Layout(dict(title = \"Price History of 5 Randomly Picked Assets\",\n",
    "                      yaxis = dict(title = 'Price (USD)'),\n",
    "                      ),legend=dict(orientation=\"h\"))\n",
    "    py.iplot(dict(data=data, layout=layout), filename='basic-line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7bbcec0d74e98cf7b83fb0f05e4ebe4b5d682649"
   },
   "source": [
    "How does the universe look? Do we see the impact of major news on the market?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "066d58a619d40beaf565c02064f2d1351fa717a8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    data = []\n",
    "    for i in [0.1, 0.25, 0.5, 0.75, 0.9]:\n",
    "        df = market_train_df.groupby('time')['close'].quantile(i).reset_index()\n",
    "\n",
    "        data.append(go.Scatter(\n",
    "            x = df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n",
    "            y = df['close'].values,\n",
    "            name = f'{i} quantile'\n",
    "        ))\n",
    "    layout = go.Layout(dict(title = \"Closing Prices in Quantiles\",\n",
    "                      yaxis = dict(title = 'Price (USD)'),\n",
    "                      ), legend=dict(orientation=\"h\"),\n",
    "        annotations=[\n",
    "            dict(\n",
    "                x='2007-02-01 00:00:00+0000',\n",
    "                y=75,\n",
    "                xref='x',\n",
    "                yref='y',\n",
    "                text='<b>Housing crisis</b>',\n",
    "                showarrow=True,\n",
    "                font=dict(\n",
    "                    size=12,\n",
    "                    color='#ffffff'\n",
    "                ),\n",
    "                align='center',\n",
    "                arrowhead=2,\n",
    "                arrowsize=1,\n",
    "                arrowwidth=2,\n",
    "                arrowcolor='#636363',\n",
    "                ax=0,\n",
    "                ay=-30,\n",
    "                borderpad=4,\n",
    "                bgcolor='#0093D1',\n",
    "                opacity=0.8\n",
    "            ),\n",
    "            dict(\n",
    "                x='2008-09-01 22:00:00+0000',\n",
    "                y=70,\n",
    "                xref='x',\n",
    "                yref='y',\n",
    "                text='<b>Collapse of Lehman Brothers</b>',\n",
    "                showarrow=True,\n",
    "                font=dict(\n",
    "                    size=12,\n",
    "                    color='#ffffff'\n",
    "                ),\n",
    "                align='center',\n",
    "                arrowhead=2,\n",
    "                arrowsize=1,\n",
    "                arrowwidth=2,\n",
    "                arrowcolor='#636363',\n",
    "                ax=0,\n",
    "                ay=-70,\n",
    "                borderpad=4,\n",
    "                bgcolor='#0093D1',\n",
    "                opacity=0.8\n",
    "            ),\n",
    "            dict(\n",
    "                x='2011-08-01 22:00:00+0000',\n",
    "                y=75,\n",
    "                xref='x',\n",
    "                yref='y',\n",
    "                text='<b>Black Monday</b>',\n",
    "                showarrow=True,\n",
    "                font=dict(\n",
    "                    size=12,\n",
    "                    color='#ffffff'\n",
    "                ),\n",
    "                align='center',\n",
    "                arrowhead=2,\n",
    "                arrowsize=1,\n",
    "                arrowwidth=2,\n",
    "                arrowcolor='#636363',\n",
    "                ax=0,\n",
    "                ay=-100,\n",
    "                borderpad=4,\n",
    "                bgcolor='#0093D1',\n",
    "                opacity=0.8\n",
    "            ),\n",
    "            dict(\n",
    "                x='2015-06-01 00:00:00+0000',\n",
    "                y=100,\n",
    "                xref='x',\n",
    "                yref='y',\n",
    "                text='<b>Stock market selloff</b>',\n",
    "                showarrow=True,\n",
    "                font=dict(\n",
    "                    size=12,\n",
    "                    color='#ffffff'\n",
    "                ),\n",
    "                align='center',\n",
    "                arrowhead=2,\n",
    "                arrowsize=1,\n",
    "                arrowwidth=2,\n",
    "                arrowcolor='#636363',\n",
    "                ax=0,\n",
    "                ay=-60,\n",
    "                borderpad=4,\n",
    "                bgcolor='#0093D1',\n",
    "                opacity=0.8\n",
    "            ),\n",
    "            dict(\n",
    "                x='2015-09-01 00:00:00+0000',\n",
    "                y=100,\n",
    "                xref='x',\n",
    "                yref='y',\n",
    "                text='<b>Oil prices crash</b>',\n",
    "                showarrow=True,\n",
    "                font=dict(\n",
    "                    size=12,\n",
    "                    color='#ffffff'\n",
    "                ),\n",
    "                align='center',\n",
    "                arrowhead=2,\n",
    "                arrowsize=1,\n",
    "                arrowwidth=2,\n",
    "                arrowcolor='#636363',\n",
    "                ax=0,\n",
    "                ay=-100,\n",
    "                borderpad=4,\n",
    "                bgcolor='#0093D1',\n",
    "                opacity=0.8\n",
    "            ),\n",
    "            dict(\n",
    "                x='2016-06-23 00:00:00+0000',\n",
    "                y=100,\n",
    "                xref='x',\n",
    "                yref='y',\n",
    "                text='<b>Brexit</b>',\n",
    "                showarrow=True,\n",
    "                font=dict(\n",
    "                    size=12,\n",
    "                    color='#ffffff'\n",
    "                ),\n",
    "                align='center',\n",
    "                arrowhead=2,\n",
    "                arrowsize=1,\n",
    "                arrowwidth=2,\n",
    "                arrowcolor='#636363',\n",
    "                ax=0,\n",
    "                ay=-30,\n",
    "                borderpad=4,\n",
    "                bgcolor='#0093D1',\n",
    "                opacity=0.8\n",
    "            )\n",
    "        ])\n",
    "    py.iplot(dict(data=data, layout=layout), filename='basic-line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f201e70f915dd7c9b976a182723b763a8eafce00"
   },
   "source": [
    "We see that some events only left a small dent on stock prices, while others had more profound impact on the market. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9dcf92f39af5e1905c63ce244c4b09a387a0db37"
   },
   "source": [
    "### Data Integrity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "00dd55351241ca2ab20f71ac77bc08c4e4e9e9e8"
   },
   "outputs": [],
   "source": [
    "market_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a21c4b16636acb7cc98caa354bb06ea989d6373f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    grouped = market_train_df.groupby('time').agg({'price_diff': ['std', 'min', 'max']}).reset_index()\n",
    "    grouped['price_diff'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "4be3e4c4cc550f67bb77eea9f42a935f6574d48b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    g = grouped.sort_values(('price_diff', 'std'), ascending=False)[:10]\n",
    "    g['min_text'] = 'Maximum price drop: ' + (g['price_diff']['min']).astype(str)\n",
    "    trace = go.Scatter(\n",
    "        x = g['time'].dt.strftime(date_format='%Y-%m-%d').values,\n",
    "        y = g['price_diff']['std'].values,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size = g['price_diff']['std'].values,\n",
    "            color = g['price_diff']['std'].values,\n",
    "            colorscale='Portland',\n",
    "            showscale=True\n",
    "        ),\n",
    "        text = g['min_text'].values\n",
    "    )\n",
    "    data = [trace]\n",
    "\n",
    "    layout= go.Layout(\n",
    "        autosize= True,\n",
    "        title= 'Top 10 Outliers of 1-Day Price Change by Month',\n",
    "        hovermode= 'closest',\n",
    "        yaxis=dict(\n",
    "            title= 'std',\n",
    "            ticklen= 5,\n",
    "            gridwidth= 2,\n",
    "        ),\n",
    "        showlegend= False\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    py.iplot(fig,filename='top10_std_chg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f4c43e1d3dc085b540af9736043089f5fb386f6b"
   },
   "source": [
    "There's no way that the price of a stock can drop by $9948.99 in a day. The others also look suspicious. Let's first find out what securities they are and then verify them through other source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "fd332dac81171201abbefcf3a42cc0a2c315d895"
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    market_train_df.sort_values('price_diff')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9fb49e6cf41f70872a78276403f3fe4d9ed87ae4"
   },
   "source": [
    "Put a ceiling and floor on these outliers with mean open or close price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "04fe6a44a65a7b66fa128f24acf6717eda1f6e20"
   },
   "outputs": [],
   "source": [
    "market_train_df['close_to_open'] =  np.abs(market_train_df['close'] / market_train_df['open'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "47fcd3e5635e68ea9681626bfb3bc9583b76fb00"
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    print(f\"Number of securities whose prices doubled in one day: {(market_train_df['close_to_open'] >= 2).sum()}.\")\n",
    "    print(f\"Number of securities whose prices fell by more then 50%: {(market_train_df['close_to_open'] <= 0.5).sum()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "19ef8496d92912fd56dce27ea0548c8a42c92212",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "market_train_df['assetName_mean_open'] = market_train_df.groupby('assetName')['open'].transform('mean')\n",
    "market_train_df['assetName_mean_close'] = market_train_df.groupby('assetName')['close'].transform('mean')\n",
    "\n",
    "# if open price is too far from mean open price for this company, replace it. Otherwise replace close price.\n",
    "for i, row in market_train_df.loc[market_train_df['close_to_open'] >= 2].iterrows():\n",
    "    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n",
    "        market_train_df.iloc[i,5] = row['assetName_mean_open']\n",
    "    else:\n",
    "        market_train_df.iloc[i,4] = row['assetName_mean_close']\n",
    "        \n",
    "for i, row in market_train_df.loc[market_train_df['close_to_open'] <= 0.5].iterrows():\n",
    "    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n",
    "        market_train_df.iloc[i,5] = row['assetName_mean_open']\n",
    "    else:\n",
    "        market_train_df.iloc[i,4] = row['assetName_mean_close']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a3f8287435af9eecb41e7d054ec52283a3cf40eb"
   },
   "source": [
    "Now let's try to rebuild the graph and make sure all the outliers are taken care."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ca35be72e7329e2265b885c2846cac73c68e0c5f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    market_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']\n",
    "    grouped = market_train_df.groupby(['time']).agg({'price_diff': ['std', 'min']}).reset_index()\n",
    "    g = grouped.sort_values(('price_diff', 'std'), ascending=False)[:10]\n",
    "    g['min_text'] = 'Maximum price drop: ' + (-1 * np.round(g['price_diff']['min'], 2)).astype(str)\n",
    "    trace = go.Scatter(\n",
    "        x = g['time'].dt.strftime(date_format='%Y-%m-%d').values,\n",
    "        y = g['price_diff']['std'].values,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size = g['price_diff']['std'].values * 5,\n",
    "            color = g['price_diff']['std'].values,\n",
    "            colorscale='Portland',\n",
    "            showscale=True\n",
    "        ),\n",
    "        text = g['min_text'].values\n",
    "    )\n",
    "    data = [trace]\n",
    "\n",
    "    layout= go.Layout(\n",
    "        autosize= True,\n",
    "        title= 'Top 10 Outliers of 1-Day Price Change by Month',\n",
    "        hovermode= 'closest',\n",
    "        yaxis=dict(\n",
    "            title= 'Std',\n",
    "            ticklen= 5,\n",
    "            gridwidth= 2,\n",
    "        ),\n",
    "        showlegend= False\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    py.iplot(fig,filename='scatter2010')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "944ef1b8fa6e155248c487540c0a21888840dca9"
   },
   "outputs": [],
   "source": [
    "market_train_df.drop(['price_diff', 'assetName_mean_open', 'assetName_mean_close'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba7e227d09cb2954e1339aab21a4c49fbe53a90e"
   },
   "source": [
    "Now let's take a look at the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "fc11eb700da57c528111c751f83bb1fc67446489",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    data = []\n",
    "    for i in [0.1, 0.25, 0.5, 0.75, 0.9]:\n",
    "        df = market_train_df.groupby('time')['returnsOpenNextMktres10'].quantile(i).reset_index()\n",
    "\n",
    "        data.append(go.Scatter(\n",
    "            x = df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n",
    "            y = df['returnsOpenNextMktres10'].values,\n",
    "            name = f'{i} quantile'\n",
    "        ))\n",
    "    layout = go.Layout(dict(title = \"returnsOpenNextMktres10 by Quantiles\",\n",
    "                      yaxis = dict(title = 'Daily Return'),\n",
    "                      ), legend=dict(orientation=\"h\"))\n",
    "    py.iplot(dict(data=data, layout=layout), filename='basic-line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "414894c26558ba77cb7e0963fba2a31c5f0d5d87"
   },
   "source": [
    "We see a wide dispersion of 10-day forward-looking market residual returns during major market events. Other times, the returns fluctuate around long-term mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "71abf4301d4c4cc18a5e300e7d20319560d49df1"
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    data = []\n",
    "    for col in ['returnsOpenNextMktres10',\n",
    "            'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n",
    "           'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n",
    "           'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n",
    "           'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']:\n",
    "        df = market_train_df.groupby('time')[col].mean().reset_index()\n",
    "        data.append(go.Scatter(\n",
    "            x = df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n",
    "            y = df[col].values,\n",
    "            name = col\n",
    "        ))\n",
    "\n",
    "    layout = go.Layout(dict(title = \"Average Returns Through Time (Before)\",\n",
    "                      yaxis = dict(title = 'Daily Return'), \n",
    "                      legend=dict(orientation=\"h\")))\n",
    "    py.iplot(dict(data=data, layout=layout), filename='basic-line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e4bbdac823114176e505b1fcd76a67959418ef0"
   },
   "source": [
    "I see a lot of people throw away data prior to 2009 because the returns were quite extreme during the financial crisis. I'm not sure whether that's the best thing to do. In my opinion, a better way to mitigate the impact of extreme data points around 2008 is to clip the data with a reasonable range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "fa25da10ee626ce5f05544f11de4172dd2f4c8d8"
   },
   "outputs": [],
   "source": [
    "market_train_df['returnsOpenNextMktres10'] = market_train_df['returnsOpenNextMktres10'].clip(-0.2, 0.2)\n",
    "market_train_df['returnsClosePrevRaw1'] = market_train_df['returnsClosePrevRaw1'].clip(-0.1, 0.1)\n",
    "market_train_df['returnsOpenPrevRaw1'] = market_train_df['returnsOpenPrevRaw1'].clip(-0.1, 0.1)\n",
    "market_train_df['returnsClosePrevMktres1'] = market_train_df['returnsClosePrevMktres1'].clip(-0.1, 0.1)\n",
    "market_train_df['returnsOpenPrevMktres1'] = market_train_df['returnsOpenPrevMktres1'].clip(-0.1, 0.1)\n",
    "market_train_df['returnsClosePrevRaw10'] = market_train_df['returnsClosePrevRaw10'].clip(-0.2, 0.2)\n",
    "market_train_df['returnsOpenPrevRaw10'] = market_train_df['returnsOpenPrevRaw10'].clip(-0.2, 0.2)\n",
    "market_train_df['returnsClosePrevMktres10'] = market_train_df['returnsClosePrevMktres10'].clip(-0.2, 0.2)\n",
    "market_train_df['returnsOpenPrevMktres10'] = market_train_df['returnsOpenPrevMktres10'].clip(-0.2, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "d8530435c569d56dba1a02b389bfc8de83a621f6"
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    data = []\n",
    "    for col in ['returnsOpenNextMktres10', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n",
    "           'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n",
    "           'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n",
    "           'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']:\n",
    "        df = market_train_df.groupby('time')[col].mean().reset_index()\n",
    "        data.append(go.Scatter(\n",
    "            x = df['time'].dt.strftime(date_format='%Y-%m-%d').values,\n",
    "            y = df[col].values,\n",
    "            name = col\n",
    "        ))\n",
    "\n",
    "    layout = go.Layout(dict(title = \"Average Returns Through Time (After)\",\n",
    "                      yaxis = dict(title = 'Price (USD)'),\n",
    "                      legend=dict(orientation=\"h\")))\n",
    "    py.iplot(dict(data=data, layout=layout), filename='basic-line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0961b6586a74be59e0e70df30ef914e776a16c83"
   },
   "source": [
    "The target variable looks much more reasonable now. This prevents extreme values from distorting our model. Let's look at the other return variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ce3cdada9c4560b8777dbbf3db5c67d1032caf0"
   },
   "source": [
    "### News Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ba97b5f5630b0cf80e47b7bdf05fb8dd5ebee870"
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    news_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "746a676f5c99038eaa2675f1c829056f8a7eba71"
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    print(news_train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d999bbf073b48992439ad1ed45e4f22c6c676ef9"
   },
   "source": [
    "Let's see the wordcloud of the first 100000 headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "abe755aae9cf82fd8eb687d3df7b0db1bcec622b"
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    text = ' '.join(news_train_df['headline'].str.lower().values[:100000])\n",
    "    wordcloud = WordCloud(max_font_size=None, stopwords=stop, background_color='white',\n",
    "                          width=1200, height=1000).generate(text)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.title('Top words in headline')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc24be952e5610d5c19465f244c141813ae7042e"
   },
   "source": [
    "Disproportionate urgency code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1de02d3bac3830091bb2eaa6d11f32c1b6b26a42"
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    (news_train_df['urgency'].value_counts() / 1000000).plot('bar');\n",
    "    plt.xticks(rotation=0);\n",
    "    plt.title('Urgency counts (million)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d0b5c75f7c6ba8f5b2669c0a5caa8680a896f62f"
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    sns.violinplot(x=news_train_df['wordCount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f8cf24be70804d617e26b6542de49b02e0fd3d72"
   },
   "source": [
    "Most of the news articles have 2500 or less words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "103ca16b4a4431358d6b49f9013d11ddb7f9452a"
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    sns.violinplot(x=news_train_df['sentenceCount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ada478042b03c13217820e4f1bfa7343affd241c"
   },
   "source": [
    "Most of the news articles have less than 100 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a06cf9147c8093acbac7d123432587e5d32410b2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    news_train_df['provider'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "19b0aeeb5a37e7de6d75d456aecb98514da77d29"
   },
   "source": [
    "It isn't surprising that Reuters is the largest news provider in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d700f29e213413d5fe3acecd9bfc9605895bca60"
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    (news_train_df['headlineTag'].value_counts() / 1000)[:10].plot('barh');\n",
    "    plt.title('headlineTag counts (thousands)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dc2cad235a461e096f0076b58b913fb5abd0d31c"
   },
   "source": [
    "Most of the news articles don't have a headline tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f3e2ebcce8656003a13575ae01908c2a61e7839"
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    pal = sns.cubehelix_palette(3, rot=-.5, dark=.3)\n",
    "    sns.violinplot(data=news_train_df[['sentimentNegative', 'sentimentNeutral', 'sentimentPositive']], palette=pal, inner=\"points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "667abac7fc0f923929d3daf9503812ac775bf868"
   },
   "source": [
    "The sentiments are bipolar, with a large portion concentrated around neutrality and a small portion clustered around the apex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "81c132598b64437c94bbf86a41a49e1527c25848",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if ANALYTICS:\n",
    "    for i, j in zip([-1, 0, 1], ['negative', 'neutral', 'positive']):\n",
    "        df_sentiment = news_train_df.loc[news_train_df['sentimentClass'] == i, 'assetName']\n",
    "        print(f'Top mentioned companies for {j} sentiment are:')\n",
    "        print(df_sentiment.value_counts().head(5))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8d1cab6278c7fdf3b70edab0e9db751a64484690"
   },
   "source": [
    "We see that a number of companies such as Apple, Citigroup, Barclays show up in more than one sentiment class. It makes sense as these large companies get more attention from the press. From time to time, they could receive very different sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8e784d89a6731a06bc75b5c0ded3730ec6d43701"
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e859c650cecaebbfcb483291f82dd92b6bfe415e"
   },
   "source": [
    "The VM doesn't have enough memory. Temporarily shrinking the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9bfec74d70371f824b5de863776a7b94b69c594"
   },
   "outputs": [],
   "source": [
    "market_train_df = market_train_df.tail(3000000)\n",
    "news_train_df = news_train_df.tail(6000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "af69c00d180e5a23438fa0fc59d72868e9f7b1aa"
   },
   "source": [
    "For market data, we extract more features from the original dataset directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "44c3af8a70c66887fbbf3b631e2c26922cc04c11"
   },
   "outputs": [],
   "source": [
    "def extract_market_features(market_train_df):\n",
    "    # downcast to reduce memory footprint\n",
    "    market_train_df['volume'] = market_train_df['volume'].astype(np.float32)\n",
    "    market_train_df['open'] = market_train_df['open'].astype(np.float32)\n",
    "    market_train_df['close'] = market_train_df['close'].astype(np.float32)\n",
    "    market_train_df['returnsClosePrevRaw1'] = market_train_df['returnsClosePrevRaw1'].astype(np.float32)\n",
    "    market_train_df['returnsClosePrevRaw10'] = market_train_df['returnsClosePrevRaw10'].astype(np.float32)\n",
    "    market_train_df['returnsClosePrevMktres1'] = market_train_df['returnsClosePrevMktres1'].astype(np.float32)\n",
    "    market_train_df['returnsClosePrevMktres10'] = market_train_df['returnsClosePrevMktres10'].astype(np.float32)\n",
    "    market_train_df['returnsOpenPrevRaw1'] = market_train_df['returnsOpenPrevRaw1'].astype(np.float32)\n",
    "    market_train_df['returnsOpenPrevRaw10'] = market_train_df['returnsOpenPrevRaw10'].astype(np.float32)\n",
    "    market_train_df['returnsOpenPrevMktres1'] = market_train_df['returnsOpenPrevMktres1'].astype(np.float32)\n",
    "    market_train_df['returnsOpenPrevMktres10'] = market_train_df['returnsOpenPrevMktres10'].astype(np.float32)\n",
    "    \n",
    "    market_train_df['close_to_open'] = market_train_df['close'] / market_train_df['open']\n",
    "    market_train_df['volume_to_mean'] = (market_train_df['volume'] / market_train_df['volume'].mean()).astype(np.float32)\n",
    "    market_train_df['returns_close_to_open_prev_raw1'] = market_train_df['returnsClosePrevRaw1'] / market_train_df['returnsOpenPrevRaw1']\n",
    "    market_train_df['returns_close_to_open_prev_raw10'] = market_train_df['returnsClosePrevRaw10'] / market_train_df['returnsOpenPrevRaw10']\n",
    "    market_train_df['returns_close_to_open_prev_mktres1'] = market_train_df['returnsClosePrevMktres1'] / market_train_df['returnsOpenPrevMktres1']\n",
    "    market_train_df['returns_close_to_open_prev_mktres10'] = market_train_df['returnsClosePrevMktres10'] / market_train_df['returnsOpenPrevMktres10']\n",
    "    market_train_df['returns_prev_open_raw1_to_open_raw10'] = market_train_df['returnsOpenPrevRaw1'] / market_train_df['returnsOpenPrevRaw10']\n",
    "    market_train_df['returns_prev_close_raw1_to_close_raw10'] = market_train_df['returnsClosePrevRaw1'] / market_train_df['returnsClosePrevRaw10']\n",
    "    market_train_df['returns_prev_open_mktres1_to_open_mktres10'] = market_train_df['returnsOpenPrevMktres1'] / market_train_df['returnsOpenPrevMktres10']\n",
    "    market_train_df['returns_prev_close_mktres1_to_close_mktres10'] = market_train_df['returnsClosePrevMktres1'] / market_train_df['returnsClosePrevMktres10']\n",
    "    \n",
    "    return market_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "deb655558bd51ef10249ed50382c506eb2c10741"
   },
   "source": [
    "* Convert \"headline\" from string to vector representation using TF-IDF.\n",
    "* Create an one-hot representation of \"subjects\" simply by counting the top 20 subjects ordered by term frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "8f336a9f9484bae9b31d33cc2b9b8caa1a4fcb2a"
   },
   "outputs": [],
   "source": [
    "def transform_headline(df, tfidf_vectorizer=None):\n",
    "    if tfidf_vectorizer:\n",
    "        headlines = tfidf_vectorizer.transform(df['headline']).toarray()\n",
    "    else:\n",
    "        tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english', ngram_range=(1, 1), \n",
    "                                           min_df=10, max_features=20, dtype=np.float32)\n",
    "        headlines = tfidf_vectorizer.fit_transform(df['headline']).toarray()\n",
    "    \n",
    "    for i in range(20):\n",
    "        column_name = 'headline' + str(i+1)\n",
    "        if (i < headlines.shape[1]):\n",
    "            df[column_name] = headlines[:,i]\n",
    "        else:\n",
    "            df[column_name] = 0\n",
    "\n",
    "    df.drop('headline', axis=1, inplace=True)\n",
    "    \n",
    "    return df, tfidf_vectorizer\n",
    "\n",
    "def transform_subjects(df, cnt_vectorizer=None):\n",
    "    if cnt_vectorizer:\n",
    "        subjects = cnt_vectorizer.transform(df['subjects']).toarray()\n",
    "    else:\n",
    "        cnt_vectorizer = CountVectorizer(lowercase=True, ngram_range=(1, 1), min_df=10, \n",
    "                                         max_features=20, dtype=bool)\n",
    "        subjects = cnt_vectorizer.fit_transform(df['subjects']).toarray()\n",
    "    \n",
    "    for i in range(20):\n",
    "        column_name = 'subject' + str(i+1)\n",
    "        if (i < subjects.shape[1]):\n",
    "            df[column_name] = subjects[:,i]\n",
    "        else:\n",
    "            df[column_name] = 0\n",
    "    \n",
    "    df.drop('subjects', axis=1, inplace=True)\n",
    "    \n",
    "    return df, cnt_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cdc6792191e4db0df58e839978050d7f0d19c670"
   },
   "source": [
    "For news data, however, we may have multiple news articles associated with a stock on a given day. We collection a number of key statistics from these articles and aggregate them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "f715922487fcc7ea70f9ebf63841250ecb4a0889"
   },
   "outputs": [],
   "source": [
    "def extract_news_features(news_train_df, tfidf_vectorizer=None, cnt_vectorizer=None):\n",
    "    news_cols_agg = {\n",
    "        'urgency': ['count'],\n",
    "        'takeSequence': ['max'],\n",
    "        'bodySize': ['mean', 'std'],\n",
    "        'wordCount': ['mean', 'std'],\n",
    "        'sentenceCount': ['mean', 'std'],\n",
    "        'companyCount': ['mean', 'std'],\n",
    "        'marketCommentary': ['mean'],\n",
    "        'relevance': ['mean', 'std'],\n",
    "        'sentimentNegative': ['min', 'max', 'mean', 'std'],\n",
    "        'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n",
    "        'sentimentPositive': ['min', 'max', 'mean', 'std'],\n",
    "        'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n",
    "        'noveltyCount12H': ['mean', 'std'],\n",
    "        'noveltyCount24H': ['mean', 'std'],\n",
    "        'noveltyCount3D': ['mean', 'std'],\n",
    "        'noveltyCount5D': ['mean', 'std'],\n",
    "        'noveltyCount7D': ['mean', 'std'],\n",
    "        'volumeCounts12H': ['mean', 'std'],\n",
    "        'volumeCounts24H': ['mean', 'std'],\n",
    "        'volumeCounts3D': ['mean', 'std'],\n",
    "        'volumeCounts5D': ['mean', 'std'],\n",
    "        'volumeCounts7D': ['mean', 'std'],\n",
    "        'headline1': ['mean'],\n",
    "        'headline2': ['mean'],\n",
    "        'headline3': ['mean'],\n",
    "        'headline4': ['mean'],\n",
    "        'headline5': ['mean'],\n",
    "        'headline6': ['mean'],\n",
    "        'headline7': ['mean'],\n",
    "        'headline8': ['mean'],\n",
    "        'headline9': ['mean'],\n",
    "        'headline10': ['mean'],\n",
    "        'headline11': ['mean'],\n",
    "        'headline12': ['mean'],\n",
    "        'headline13': ['mean'],\n",
    "        'headline14': ['mean'],\n",
    "        'headline15': ['mean'],\n",
    "        'headline16': ['mean'],\n",
    "        'headline17': ['mean'],\n",
    "        'headline18': ['mean'],\n",
    "        'headline19': ['mean'],\n",
    "        'headline20': ['mean'],\n",
    "        'subject1': ['mean'],\n",
    "        'subject2': ['mean'],\n",
    "        'subject3': ['mean'],\n",
    "        'subject4': ['mean'],\n",
    "        'subject5': ['mean'],\n",
    "        'subject6': ['mean'],\n",
    "        'subject7': ['mean'],\n",
    "        'subject8': ['mean'],\n",
    "        'subject9': ['mean'],\n",
    "        'subject10': ['mean'],\n",
    "        'subject11': ['mean'],\n",
    "        'subject12': ['mean'],\n",
    "        'subject13': ['mean'],\n",
    "        'subject14': ['mean'],\n",
    "        'subject15': ['mean'],\n",
    "        'subject16': ['mean'],\n",
    "        'subject17': ['mean'],\n",
    "        'subject18': ['mean'],\n",
    "        'subject19': ['mean'],\n",
    "        'subject20': ['mean']\n",
    "    }\n",
    "    \n",
    "    # text data -> vector representation\n",
    "    news_train_df, tfidf_vectorizer = transform_headline(news_train_df, tfidf_vectorizer)\n",
    "    news_train_df, cnt_vectorizer = transform_subjects(news_train_df, cnt_vectorizer)\n",
    "    \n",
    "    # Fix asset codes (str -> list)\n",
    "    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\./]+)'\")    \n",
    "    \n",
    "    # Expand assetCodes\n",
    "    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n",
    "    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n",
    "\n",
    "    assert len(assetCodes_index) == len(assetCodes_expanded)\n",
    "    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n",
    "\n",
    "    # Create expandaded news (will repeat every assetCodes' row)\n",
    "    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n",
    "    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n",
    "\n",
    "    # Free memory\n",
    "    del news_train_df, df_assetCodes\n",
    "\n",
    "    # Aggregate numerical news features\n",
    "    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n",
    "    \n",
    "    # Free memory\n",
    "    del news_train_df_expanded\n",
    "\n",
    "    # Convert to float32 to save memory\n",
    "    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n",
    "\n",
    "    # Flat columns\n",
    "    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n",
    "    \n",
    "    return news_train_df_aggregated, tfidf_vectorizer, cnt_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "57973e95286b66b3f475859c110d12db37873609"
   },
   "outputs": [],
   "source": [
    "def merge_market_news(market_train_df, news_train_df):\n",
    "    market_train_df = market_train_df.join(news_train_df, on=['time', 'assetCode'])\n",
    "    \n",
    "    return market_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "e0f67ef5d6dc24c93f3d998782bad5f03a5f0d26"
   },
   "outputs": [],
   "source": [
    "def label_encode(series, min_count):\n",
    "    vc = series.value_counts()\n",
    "    le = {c:i for i, c in enumerate(vc.index[vc >= min_count])}\n",
    "    return le\n",
    "\n",
    "def get_xy(market_train_df, news_train_df, le=None, tfidf_vectorizer=None, cnt_vectorizer=None):\n",
    "    # Split date into before and after 22h (the time used in train data)\n",
    "    # E.g: 2007-03-07 23:26:39+00:00 -> 2007-03-08 00:00:00+00:00 (next day)\n",
    "    #      2009-02-25 21:00:50+00:00 -> 2009-02-25 00:00:00+00:00 (current day)\n",
    "    news_train_df['time'] = (pd.to_datetime(news_train_df['time']) - np.timedelta64(22,'h')).dt.ceil('1D')\n",
    "\n",
    "    # Round time of market_train_df to 0h of curret day\n",
    "    market_train_df['time'] = pd.to_datetime(market_train_df['time']).dt.floor('1D')\n",
    "\n",
    "    market_train_df = extract_market_features(market_train_df)\n",
    "    news_train_df, tfidf_vectorizer, cnt_vectorizer = extract_news_features(news_train_df, \n",
    "                                                                            tfidf_vectorizer, \n",
    "                                                                            cnt_vectorizer)\n",
    "    market_train_df = merge_market_news(market_train_df, news_train_df)\n",
    "\n",
    "    # If not label-encoder... encode assetCode\n",
    "    if le is None:\n",
    "        le_assetCode = label_encode(market_train_df['assetCode'], min_count=10)\n",
    "        le_assetName = label_encode(market_train_df['assetName'], min_count=5)\n",
    "    else:\n",
    "        # 'unpack' label encoders\n",
    "        le_assetCode, le_assetName = le\n",
    "        \n",
    "    market_train_df['assetCode'] = market_train_df['assetCode'].map(le_assetCode).fillna(-1).astype(np.int16)\n",
    "    market_train_df['assetName'] = market_train_df['assetName'].map(le_assetName).fillna(-1).astype(np.int16)\n",
    "    market_train_df['dayofweek'] = market_train_df.time.dt.dayofweek.astype(np.int8)\n",
    "    market_train_df['month'] = market_train_df.time.dt.month.astype(np.int8)\n",
    "    \n",
    "    try:\n",
    "        y = market_train_df['returnsOpenNextMktres10'].astype(np.float32)\n",
    "        market_train_df.drop(columns=['returnsOpenNextMktres10'], inplace=True)\n",
    "    except:\n",
    "        y = None\n",
    "    \n",
    "    try:\n",
    "        time = market_train_df['time']\n",
    "        market_train_df.drop(columns='time', inplace=True)\n",
    "    except:\n",
    "        time = None\n",
    "    \n",
    "    try:\n",
    "        universe = market_train_df['universe']\n",
    "        market_train_df.drop(columns=['universe'], inplace=True)\n",
    "    except:\n",
    "        universe = None\n",
    "    \n",
    "    return market_train_df, y, time, universe, (le_assetCode, le_assetName), tfidf_vectorizer, cnt_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3c741c3ad43c40d0be7acb177c11b4303337d89"
   },
   "source": [
    "We then merge market data and news data, and remove the original dataframes to reduce memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "3d1384ebeba95964425e8e2d73e2850f335c0904"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-1cfbd1e4a591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmarket_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarket_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_train_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mnews_train_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-65f603edfa2b>\u001b[0m in \u001b[0;36mget_xy\u001b[0;34m(market_train_df, news_train_df, le, tfidf_vectorizer, cnt_vectorizer)\u001b[0m\n\u001b[1;32m     16\u001b[0m     news_train_df, tfidf_vectorizer, cnt_vectorizer = extract_news_features(news_train_df, \n\u001b[1;32m     17\u001b[0m                                                                             \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                                                                             cnt_vectorizer)\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mmarket_train_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_market_news\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarket_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnews_train_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-7b42bf3d8ebf>\u001b[0m in \u001b[0;36mextract_news_features\u001b[0;34m(news_train_df, tfidf_vectorizer, cnt_vectorizer)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# text data -> vector representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mnews_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_headline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mnews_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_subjects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt_vectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-aa9b9e328255>\u001b[0m in \u001b[0;36mtransform_headline\u001b[0;34m(df, tfidf_vectorizer)\u001b[0m\n\u001b[1;32m      5\u001b[0m         tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english', ngram_range=(1, 1), \n\u001b[1;32m      6\u001b[0m                                            min_df=10, max_features=20, dtype=np.float32)\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mheadlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'headline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \"\"\"\n\u001b[0;32m-> 1381\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[1;32m    120\u001b[0m                              \"unicode string.\")\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "market_train_df, y, time, universe, le, tfidf_vectorizer, cnt_vectorizer = get_xy(market_train_df, news_train_df)\n",
    "\n",
    "del news_train_df\n",
    "gc.collect()\n",
    "\n",
    "print(market_train_df.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6728e897575adc06873530ada10650a2f5b74e3d"
   },
   "source": [
    "Determine what features to use for training and normalized training data: (No need to normalize data for tree based model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "63e7230e0fb960dd9e656ed83aef8e9f1a90ec9e"
   },
   "outputs": [],
   "source": [
    "feature_cols = market_train_df.columns.tolist()\n",
    "# feature_cols = [c for c in X.columns if c not in ['time', 'universe', 'returnsOpenNextMktres10']]\n",
    "\n",
    "# X = X[feature_cols]\n",
    "\n",
    "# Normalize X values\n",
    "# mins = np.nanmin(X[X != np.inf], axis=0)\n",
    "# maxs = np.nanmax(X[X != np.inf], axis=0)\n",
    "# rng = maxs - mins\n",
    "# X = 1 - ((maxs - X) / rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c6fff80b41ae5b6609192cb943a3c70de5f3c76"
   },
   "outputs": [],
   "source": [
    "market_train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6857e16b9f52eb381b7282362c0de83885a1c943"
   },
   "source": [
    "Create training and validation datasets and add time and universe to validation dataset in order to calculate custom scores later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a83014f9e29345ead77f5ea4b178708ca436bfb"
   },
   "source": [
    "**XGBClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f730540135f83ca2c82ab7104e66237dc8b408c"
   },
   "outputs": [],
   "source": [
    "# up = y >= 0\n",
    "# up = up.values\n",
    "# r = y.values\n",
    "# u = universe\n",
    "# day = time.dt.date\n",
    "\n",
    "# Scaling of X values\n",
    "# It is good to keep these scaling values for later\n",
    "# mins = np.min(X, axis=0)\n",
    "# maxs = np.max(X, axis=0)\n",
    "# rng = maxs - mins\n",
    "# X = 1 - ((maxs - X) / rng)\n",
    "\n",
    "# X_train, X_test, up_train, up_test, r_train, r_test, u_train, u_test, d_train, d_test\\\n",
    "# = train_test_split(X, up, r, u, day, test_size=0.25, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "276bd4cec2c617d7ab0513bc4546b7332a61f34e"
   },
   "outputs": [],
   "source": [
    "# Create XGB Classifier Model and fit to data\n",
    "# from xgboost import XGBClassifier\n",
    "# import time\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# xgb_model = XGBClassifier(n_jobs=4,n_estimators=200,max_depth=8,eta=0.1)\n",
    "\n",
    "# t = time.time()\n",
    "# print('Training XGB Model')\n",
    "# xgb_model.fit(X_train, up_train)\n",
    "# print(f'Done, time = {time.time() - t}')\n",
    "\n",
    "# accuracy_score(xgb_model.predict(X_test), up_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "808c5f2192e98d3c7c084d7a213a6acaa668a834"
   },
   "outputs": [],
   "source": [
    "# Print XGB UP Confidence Graph\n",
    "# confidence_test = xgb_model.predict_proba(X_test)[:,1]*2 -1\n",
    "# plt.hist(confidence_test, bins='auto')\n",
    "# plt.title(\"XGB predicted confidence\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fddee51a514a79e2bf80ddcc8db053f6848df655"
   },
   "outputs": [],
   "source": [
    "# Calculate Final Score Metric for XGB\n",
    "# r_test = r_test.clip(-1,1) # get rid of outliers. Where do they come from??\n",
    "# x_t_i = confidence_test * r_test * u_test\n",
    "# data = {'day' : d_test, 'x_t_i' : x_t_i}\n",
    "# df = pd.DataFrame(data)\n",
    "# x_t = df.groupby('day').sum().values.flatten()\n",
    "# mean = np.mean(x_t)\n",
    "# std = np.std(x_t)\n",
    "# score_test = mean / std\n",
    "# print(f'XGBoost Up score: {score_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "559c0d07ec920ad675645b88110174438167284a"
   },
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "# plt.figure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "# plt.bar(range(len(xgb_model.feature_importances_)), xgb_model.feature_importances_)\n",
    "# plt.title(\"XGB Feature Importance\")\n",
    "# plt.xticks(range(len(xgb_model.feature_importances_)), feature_cols, rotation='vertical');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ef83915bf68bc754c2a0c93c5865f5762db6d0a1"
   },
   "source": [
    "**CatBoost Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c06f4570518e938ea88bc80dcd7177e5dbdb91a"
   },
   "outputs": [],
   "source": [
    "# cat_model = CatBoostClassifier(thread_count=4, n_estimators=200, max_depth=10, \n",
    "#                                eta=0.1, loss_function='Logloss' , verbose=10)\n",
    "\n",
    "# t = time.time()\n",
    "# print('Fitting CatBoost Model')\n",
    "# cat_model.fit(X_train, up_train)\n",
    "# print(f'cat Done, time = {time.time() - t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b1721bc2d3f3e03fee010a32bf8fcfd0e75b3db2"
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score\n",
    "# catconfidence_test = cat_model.predict_proba(X_test)[:,1]*2 -1\n",
    "\n",
    "# print(accuracy_score(catconfidence_test>0,up_test))\n",
    "# plt.hist(catconfidence_test, bins='auto')\n",
    "# plt.title(\"CatBoost predicted confidence\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c65ef871219dd913a5c4a0948e05f532b1fd126"
   },
   "outputs": [],
   "source": [
    "# x_t_icat = catconfidence_test * r_test * u_test\n",
    "# data2 = {'day' : d_test, 'x_t_icat' : x_t_icat}\n",
    "# df2 = pd.DataFrame(data2)\n",
    "# x_tcat = df2.groupby('day').sum().values.flatten()\n",
    "# mean = np.mean(x_tcat)\n",
    "# std = np.std(x_tcat)\n",
    "# score_testcat = mean / std\n",
    "# print(f'CatBoost score: {score_testcat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "70d2dc191c1bff384d0dcfc9ab914b178fa4d6f4"
   },
   "outputs": [],
   "source": [
    "# # free up memory\n",
    "# del X_train, X_test, up_train, up_test, r_train, r_test, u_train, u_test, d_train, d_test\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5a09908383118de186fd3bb2ccf20085743bac21"
   },
   "source": [
    "**LightGBM Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "612cfb17a83c2e15da306895c03aacef72515365"
   },
   "outputs": [],
   "source": [
    "# train_index, test_index = train_test_split(market_train_df.index, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e0c77b1812643d23a7ce31ce3aae67605c1f9db"
   },
   "source": [
    "Use LightGBM to build a regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b99729d65146107b848d4e983147536ef4de9c62"
   },
   "outputs": [],
   "source": [
    "def score_function(preds, valid_data):\n",
    "    time = valid_data.params['time']\n",
    "    universe = valid_data.params['universe']\n",
    "    labels = valid_data.get_label()\n",
    "\n",
    "    x_t = preds * labels * universe\n",
    "    x_t_sum = x_t.groupby(time).sum()\n",
    "    score = x_t_sum.mean() / x_t_sum.std()\n",
    "    \n",
    "    return 'score', score, True\n",
    "\n",
    "def evaluate_model(X, y, train_index, test_index, params):\n",
    "    data_train = lgb.Dataset(X.loc[train_index].values, y.loc[train_index], feature_name=feature_cols, free_raw_data=False)\n",
    "    data_test = lgb.Dataset(X.loc[test_index].values, y.loc[test_index], feature_name=feature_cols, free_raw_data=False)\n",
    "    data_test.params = {\n",
    "        'time': time.loc[test_index].factorize()[0],\n",
    "        'universe': universe.loc[test_index].values\n",
    "    }\n",
    "    evals_result = {}\n",
    "    \n",
    "    model = lgb.train(params, train_set=data_train, num_boost_round=2000,\n",
    "                      valid_sets=(data_test,), valid_names=('test',),\n",
    "                      verbose_eval=50, early_stopping_rounds=100,\n",
    "                      feval=score_function, evals_result=evals_result)\n",
    "\n",
    "    df_result = pd.DataFrame(evals_result['test'])\n",
    "    \n",
    "    return df_result, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8652390c7e826d1b99903a0d15b568df057a59bd"
   },
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65a62e8a7053ad26bc31659cfffd9da1a52558b2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_bayesian_optimization():\n",
    "    # optimize params in these ranges\n",
    "    spaces = [\n",
    "        (0.05, 0.22), #learning_rate\n",
    "        (800, 2000), #num_leaves\n",
    "        (500, 3000), #min_data_in_leaf\n",
    "        (6, 15), # max_depth\n",
    "        (0.5, 1.0), # bagging_fraction\n",
    "        (1, 5), # bagging_freq\n",
    "        (0.5, 1.0), # feature_fraction\n",
    "        (200, 800), #max_bin\n",
    "        (0.3, 0.9), # lambda_l1\n",
    "        (0.3, 0.9) # lambda_l2\n",
    "    ]\n",
    "\n",
    "    def f(x):\n",
    "        lgb_param = {\n",
    "            'boosting': 'gbdt', \n",
    "            'objective': 'regression_l1',\n",
    "            'metric': 'None',\n",
    "            'learning_rate': x[0],\n",
    "            'num_leaves': x[1],\n",
    "            'min_data_in_leaf': x[2],\n",
    "            'max_depth': x[3],\n",
    "            'bagging_fraction': x[4],\n",
    "            'bagging_freq': x[5],\n",
    "            'feature_fraction': x[6],\n",
    "            'max_bin': x[7],\n",
    "            'lambda_l1': x[8],\n",
    "            'lambda_l2': x[9],\n",
    "            'seed': 2018\n",
    "        }\n",
    "\n",
    "        df_result, model = evaluate_model(market_train_df, y, train_index,test_index, lgb_param)\n",
    "\n",
    "        return -df_result['score'].values[-1]\n",
    "\n",
    "    # run optimization\n",
    "    from skopt import gp_minimize\n",
    "    res = gp_minimize(\n",
    "        f, spaces,\n",
    "        acq_func=\"EI\",\n",
    "        n_calls=15)\n",
    "\n",
    "    # print tuned params\n",
    "    print(res.x)\n",
    "\n",
    "    # plot tuning process\n",
    "    from skopt.plots import plot_convergence\n",
    "    plot_convergence(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ea02fb1f666476b718e85d34c62d8869e41dbcc5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run_bayesian_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dd1501a02848dbb03e9b4b49190a6cd47fc8491d"
   },
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'boosting': 'gbdt', \n",
    "    'objective': 'regression_l1',\n",
    "    'metric': 'None',\n",
    "    'learning_rate': 0.22,\n",
    "    'num_leaves': 2000,\n",
    "    'min_data_in_leaf': 500,\n",
    "    'max_depth': 15,\n",
    "    'bagging_fraction': 1.0,\n",
    "    'bagging_freq': 1,\n",
    "    'feature_fraction': 1.0,\n",
    "    'max_bin': 200,\n",
    "    'lambda_l1': 0.30,\n",
    "    'lambda_l2': 0.90,\n",
    "    'seed': 2018\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5190fffb5a49859564ac76fcd5cbace09fce50da"
   },
   "outputs": [],
   "source": [
    "# df_result, model = evaluate_model(market_train_df, y, train_index, test_index, lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "210c685b6d5c3222d80475d5d2e34b135a3b48d7"
   },
   "outputs": [],
   "source": [
    "# ax = df_result.plot(figsize=(12, 8))\n",
    "# ax.scatter(df_result['score'].idxmax(), df_result['score'].max())\n",
    "# ax.set_xlabel(\"iteration\")\n",
    "# ax.set_ylabel(\"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4826395ee5ab137f19b37a93db8221ad4f6705b6"
   },
   "outputs": [],
   "source": [
    "# num_boost_round, valid_score = df_result['score'].idxmax()+1, df_result['score'].max()\n",
    "# print(lgb_params)\n",
    "# print(f'Best score was {valid_score:.5f} on round {num_boost_round}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "23f193f5ef2183a4ef04494ce0715f9b1e4011fc"
   },
   "source": [
    "We will use this optimized hyperparameter to train the full model later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f8dedf18e062131e6e4ffd2bdf669ad71004a732"
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 2, figsize=(16, 20))\n",
    "# lgb.plot_importance(model, ax=ax[0], title='Feature Importance (split)', xlabel='Feature importance', ylabel='Features',\n",
    "#                     importance_type='split', max_num_features=114)\n",
    "# lgb.plot_importance(model, ax=ax[1], title='Feature Importance (gain)', xlabel='Feature importance', ylabel='Features',\n",
    "#                     importance_type='gain', max_num_features=114)\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "49b2920f13f9aca15797dbf2c0976cdf56225a64",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lgb.plot_tree(model, figsize=(16, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "617ec8e0837f7e1df5386143796c01d0e17a8403"
   },
   "outputs": [],
   "source": [
    "# free up memory\n",
    "del time, universe\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e3f3b28720bad2225c1cc0c4fcbc98f7736c2d5"
   },
   "source": [
    "## Train Full Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "48c72e7890f4df0c529c28268fdfb79392679bd4"
   },
   "source": [
    "#### Cat Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ccd3e208a4219d6769f6dca58c6ed9be3d8461dd"
   },
   "outputs": [],
   "source": [
    "# # Final Model\n",
    "# catModel = CatBoostClassifier(thread_count=4, n_estimators=200, max_depth=10, eta=0.1, loss_function='Logloss' , verbose=10)\n",
    "\n",
    "# t = time.time()\n",
    "# print('Training Model')\n",
    "# catModel.fit(market_train_df, up)\n",
    "# print(f'Done, time = {time.time() - t}')\n",
    "\n",
    "# del up\n",
    "# gc.collect()\n",
    "\n",
    "# # data_train_full = lgb.Dataset(market_train_df, y, feature_name=feature_cols)\n",
    "# del market_train_df, y\n",
    "# gc.collect()\n",
    "# # lgbModel = lgb.train(lgb_params, train_set=data_train_full, num_boost_round=num_boost_round)\n",
    "# # del data_train_full\n",
    "# # gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c58c5d6ba795778013867c55c80b0ff41e438774"
   },
   "outputs": [],
   "source": [
    "# def make_predictions(market_obs_df, news_obs_df, predictions_template_df, le):\n",
    "#     X, y, time, universe, le, tfidf_vectorizer, cnt_vectorizer = get_xy(market_obs_df, news_obs_df, le,\n",
    "#                                                                         tfidf_vectorizer, cnt_vectorizer) \n",
    "    \n",
    "#     catPrediction = np.clip(catModel.predict_proba(X)[:,1]*2 -1, -1, 1)\n",
    "# #     lgbPrediction = np.clip(lgbModel.predict(X), -1, 1)\n",
    "#     \n",
    "# #     prediction = (catPrediction + lgbPrediction) / 2\n",
    "#     prediction = catPrediction\n",
    "#     predictions_template_df.confidenceValue = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "60afed752bfcd7db3ffe05f3d77df32b1a13a949"
   },
   "outputs": [],
   "source": [
    "# days = env.get_prediction_days()\n",
    "\n",
    "# for (market_obs_df, news_obs_df, predictions_template_df) in days:\n",
    "#     make_predictions(market_obs_df, news_obs_df, predictions_template_df, le)\n",
    "#     env.predict(predictions_template_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bf59f822a84f3fafe0872ff3c505a98a7371e65d"
   },
   "outputs": [],
   "source": [
    "# env.write_submission_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07acce9032fd70009b5cfc02285898a91f4b14a8"
   },
   "source": [
    "**LightGBM Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2cd714f308b7d625031b52995327e7cb3c104ca8"
   },
   "outputs": [],
   "source": [
    "num_boost_round = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "392a1d9fdcfe929c863babec007d751e8b332c3b"
   },
   "outputs": [],
   "source": [
    "train_index, test_index = train_test_split(market_train_df.index, test_size=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f918b17a8cd8de8e17cb1f5c98058a4b5d6f1075"
   },
   "outputs": [],
   "source": [
    "data_train_full = lgb.Dataset(market_train_df.loc[train_index].values, y.loc[train_index], feature_name=feature_cols, free_raw_data=False)\n",
    "del market_train_df, y\n",
    "gc.collect()\n",
    "model = lgb.train(lgb_params, train_set=data_train_full, num_boost_round=num_boost_round)\n",
    "del data_train_full\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a791baec936c9d00eada16ebf97cf7ab5927faf5"
   },
   "outputs": [],
   "source": [
    "def make_predictions(market_obs_df, news_obs_df, predictions_template_df, le, tfidf_vectorizer, cnt_vectorizer):\n",
    "    X, y, time, universe, le, tfidf_vectorizer, cnt_vectorizer = get_xy(market_obs_df, news_obs_df, le, \n",
    "                                                                        tfidf_vectorizer, cnt_vectorizer)\n",
    "    # X_new = X[feature_cols].values\n",
    "    # X_new = 1 - ((maxs - X_new) / rng)\n",
    "    \n",
    "    predictions_template_df.confidenceValue = np.clip(model.predict(X), -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b7b548b08eb281b4a341d235cbeea8b7e1201a36",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "days = env.get_prediction_days()\n",
    "\n",
    "for (market_obs_df, news_obs_df, predictions_template_df) in days:\n",
    "    make_predictions(market_obs_df, news_obs_df, predictions_template_df, le, tfidf_vectorizer, cnt_vectorizer)\n",
    "    env.predict(predictions_template_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "12a65ba92ed0f1185a8c4b30c0f8142da4ec8418"
   },
   "outputs": [],
   "source": [
    "env.write_submission_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ee96faf0644ae6b7bd069d2b677a010ca2b919ce"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
